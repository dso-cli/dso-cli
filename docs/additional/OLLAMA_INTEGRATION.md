# üß† Int√©gration Ollama - Documentation Technique

## Vue d'ensemble

DSO utilise Ollama pour l'analyse IA locale des r√©sultats de scan. L'int√©gration est compl√®te et robuste avec support de l'API chat moderne.

## Fonctionnalit√©s

### ‚úÖ API Chat Moderne

DSO utilise l'API `/api/chat` d'Ollama (au lieu de `/api/generate`) pour :
- Meilleure gestion du contexte
- Support des conversations multi-tours
- Format plus standardis√©

### ‚úÖ Streaming (optionnel)

Support du streaming pour afficher la progression en temps r√©el :

```go
client.GenerateStream(prompt, func(chunk string) {
    fmt.Print(chunk) // Affiche au fur et √† mesure
})
```

### ‚úÖ Gestion d'erreurs robuste

- Retry automatique pour les connexions
- Messages d'erreur clairs avec solutions
- V√©rification pr√©alable de la disponibilit√©

### ‚úÖ Configuration flexible

- **Mod√®le** : Variable `DSO_MODEL` (d√©faut: `llama3.1:8b`)
- **URL** : Variable `OLLAMA_HOST` (d√©faut: `http://localhost:11434`)
- D√©tection automatique des mod√®les disponibles

### ‚úÖ T√©l√©chargement automatique

Si le mod√®le configur√© n'est pas install√©, DSO le t√©l√©charge automatiquement avec affichage de la progression.

## Architecture

```
cmd/audit.go
    ‚Üì
internal/llm/prompts.go (Analyze)
    ‚Üì
internal/llm/ollama.go (OllamaClient)
    ‚Üì
Ollama API (/api/chat)
```

## Utilisation

### Client de base

```go
client := llm.NewOllamaClient()
response, err := client.Generate("Analyse ces r√©sultats...")
```

### Avec contexte

```go
context := []map[string]string{
    {"role": "system", "content": "Tu es un expert s√©curit√©"},
    {"role": "user", "content": "Question pr√©c√©dente"},
}
response, err := client.GenerateWithContext("Nouvelle question", context)
```

### Streaming

```go
response, err := client.GenerateStream(prompt, func(chunk string) {
    fmt.Print(chunk)
    os.Stdout.Sync()
})
```

## Commandes CLI

### `dso check`

V√©rifie l'√©tat d'Ollama :
- Connexion
- Mod√®les disponibles
- Mod√®le configur√© install√©

### `dso audit .`

Utilise automatiquement Ollama pour analyser les r√©sultats.

## D√©pannage

### Ollama non accessible

```bash
# V√©rifier que Ollama tourne
ollama serve

# V√©rifier la connexion
dso check
```

### Mod√®le non trouv√©

```bash
# Lister les mod√®les
ollama list

# T√©l√©charger le mod√®le
ollama pull llama3.1:8b
```

### Timeout

Les timeouts sont configur√©s √† 5 minutes par d√©faut. Pour les analyses tr√®s longues, augmente la valeur dans `ollama.go`.

## Am√©liorations futures

- [ ] Cache des r√©ponses pour √©viter les appels r√©p√©t√©s
- [ ] Support de plusieurs mod√®les en parall√®le
- [ ] M√©triques de performance (latence, tokens/s)
- [ ] Support des mod√®les externes (OpenAI, Anthropic) en fallback

